{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOuS6+XobtWaYVYFqhrymmB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MOZOmEoOjJmm"},"outputs":[],"source":["using_colab = True"]},{"cell_type":"code","source":["if using_colab:\n","    import torch\n","    import torchvision\n","    print(\"PyTorch version:\", torch.__version__)\n","    print(\"Torchvision version:\", torchvision.__version__)\n","    print(\"CUDA is available:\", torch.cuda.is_available())\n","    import sys\n","    !{sys.executable} -m pip install opencv-python matplotlib\n","    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n","\n","    !mkdir -p videos\n","    !wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n","    !unzip -d videos videos/bedroom.zip\n","\n","    !mkdir -p ../checkpoints/\n","    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"],"metadata":{"id":"Yb20ykFgjQta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","# if using Apple MPS, fall back to CPU for unsupported ops\n","os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","from PIL import Image"],"metadata":{"id":"Uk6pkMUKjVPc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select the device for computation\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","elif torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","else:\n","    device = torch.device(\"cpu\")\n","print(f\"using device: {device}\")\n","\n","if device.type == \"cuda\":\n","    # use bfloat16 for the entire notebook\n","    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n","    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n","    if torch.cuda.get_device_properties(0).major >= 8:\n","        torch.backends.cuda.matmul.allow_tf32 = True\n","        torch.backends.cudnn.allow_tf32 = True\n","elif device.type == \"mps\":\n","    print(\n","        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n","        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n","        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n","    )"],"metadata":{"id":"V57XhljmjWYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sam2.build_sam import build_sam2_video_predictor\n","\n","sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n","model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n","\n","predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"],"metadata":{"id":"neJ8ydOmjX0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_mask(mask, ax, obj_id=None, random_color=False):\n","    if random_color:\n","        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n","    else:\n","        cmap = plt.get_cmap(\"tab10\")\n","        cmap_idx = 0 if obj_id is None else obj_id\n","        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n","    h, w = mask.shape[-2:]\n","    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n","    ax.imshow(mask_image)\n","\n","\n","def show_points(coords, labels, ax, marker_size=200):\n","    pos_points = coords[labels==1]\n","    neg_points = coords[labels==0]\n","    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n","    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n","\n","\n","def show_box(box, ax):\n","    x0, y0 = box[0], box[1]\n","    w, h = box[2] - box[0], box[3] - box[1]\n","    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"],"metadata":{"id":"-guEFGQ5jZFy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","import zipfile\n","import os\n","\n","# Upload the zip file\n","uploaded = files.upload()  # You'll be prompted to upload a .zip file\n","\n","# Extract the zip\n","for fname in uploaded.keys():\n","    if fname.endswith('.zip'):\n","        with zipfile.ZipFile(fname, 'r') as zip_ref:\n","            zip_ref.extractall('extracted_folder')  # You can rename this\n","            print(f\"Extracted to 'extracted_folder'\")\n"],"metadata":{"id":"ZBEH8n2ZjbBn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["change this with file path --video_dir = \"/content/extracted_folder/frames4\""],"metadata":{"id":"LzPQnSdNjfB8"}},{"cell_type":"code","source":["# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n","video_dir = \"/content/extracted_folder/frames4\"\n","\n","# scan all the JPEG frame names in this directory\n","frame_names = [\n","    p for p in os.listdir(video_dir)\n","    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n","]\n","frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n","\n","# take a look the first video frame\n","frame_idx = 0\n","plt.figure(figsize=(9, 6))\n","plt.title(f\"frame {frame_idx}\")\n","plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"],"metadata":{"id":"nsmV_daUjbfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inference_state = predictor.init_state(video_path=video_dir)"],"metadata":{"id":"eoah39fjjnNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictor.reset_state(inference_state)"],"metadata":{"id":"tpHDWMzzjpEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ann_frame_idx = 0  # the frame index we interact with\n","ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n","\n","# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n","box = np.array([40,0, 750, 750], dtype=np.float32)\n","_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n","    inference_state=inference_state,\n","    frame_idx=ann_frame_idx,\n","    obj_id=ann_obj_id,\n","    box=box,\n",")\n","\n","# show the results on the current (interacted) frame\n","plt.figure(figsize=(9, 6))\n","plt.title(f\"frame {ann_frame_idx}\")\n","plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n","show_box(box, plt.gca())\n","show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"],"metadata":{"id":"GHDGKnqGjwEP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ann_frame_idx = 0  # the frame index we interact with\n","ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n","\n","# Let's add a positive click at (x, y) = (460, 60) to refine the mask\n","points = np.array([[100, 450]], dtype=np.float32)\n","# for labels, `1` means positive click and `0` means negative click\n","labels = np.array([1], np.int32)\n","# note that we also need to send the original box input along with\n","# the new refinement click together into `add_new_points_or_box`\n","box = np.array([40,0, 750, 750], dtype=np.float32)\n","_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n","    inference_state=inference_state,\n","    frame_idx=ann_frame_idx,\n","    obj_id=ann_obj_id,\n","    points=points,\n","    labels=labels,\n","    box=box,\n",")\n","\n","# show the results on the current (interacted) frame\n","plt.figure(figsize=(9, 6))\n","plt.title(f\"frame {ann_frame_idx}\")\n","plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n","show_box(box, plt.gca())\n","show_points(points, labels, plt.gca())\n","show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"],"metadata":{"id":"aSLjhJYqjydY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run propagation throughout the video and collect the results in a dict\n","video_segments = {}  # video_segments contains the per-frame segmentation results\n","for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n","    video_segments[out_frame_idx] = {\n","        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n","        for i, out_obj_id in enumerate(out_obj_ids)\n","    }\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from google.colab import files\n","\n","# Create output folder\n","output_dir = \"/content/output_frames\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Parameters\n","vis_frame_stride = 5\n","output_video_path = \"/content/segmentation_output.mp4\"\n","\n","# Save each masked frame\n","plt.close(\"all\")\n","for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n","    fig = plt.figure(figsize=(6, 4))\n","    plt.title(f\"frame {out_frame_idx}\")\n","\n","    img = Image.open(os.path.join(video_dir, frame_names[out_frame_idx]))\n","    plt.imshow(img)\n","\n","    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n","\n","    save_path = os.path.join(output_dir, f\"frame_{out_frame_idx:04d}.png\")\n","    fig.savefig(save_path)\n","    plt.close(fig)\n","\n","# Create video from saved frames\n","image_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.png')])\n","first_frame = cv2.imread(os.path.join(output_dir, image_files[0]))\n","height, width, _ = first_frame.shape\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","video_writer = cv2.VideoWriter(output_video_path, fourcc, 10, (width, height))\n","\n","for image_file in image_files:\n","    frame = cv2.imread(os.path.join(output_dir, image_file))\n","    video_writer.write(frame)\n","video_writer.release()\n","\n","# Download the video\n","files.download(output_video_path)\n","\n","\n","\n","# render the segmentation results every few frames\n","vis_frame_stride = 30\n","plt.close(\"all\")\n","for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n","    plt.figure(figsize=(6, 4))\n","    plt.title(f\"frame {out_frame_idx}\")\n","    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n","    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"],"metadata":{"id":"MwI3ce2jj0MA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["IF u wish to identify multiple objects"],"metadata":{"id":"f6gFLXfpj3RK"}},{"cell_type":"code","source":["predictor.reset_state(inference_state)"],"metadata":{"id":"3Q3Ho3iSj9_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompts = {}  # hold all the clicks we add for visualization"],"metadata":{"id":"5IFUktJVkAg6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ann_frame_idx = 0  # the frame index we interact with\n","ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n","\n","# Let's add a positive click at (x, y) = (200, 300) to get started on the first object\n","points = np.array([[150, 300]], dtype=np.float32)\n","# for labels, `1` means positive click and `0` means negative click\n","labels = np.array([1], np.int32)\n","prompts[ann_obj_id] = points, labels\n","_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n","    inference_state=inference_state,\n","    frame_idx=ann_frame_idx,\n","    obj_id=ann_obj_id,\n","    points=points,\n","    labels=labels,\n",")\n","\n","# show the results on the current (interacted) frame\n","plt.figure(figsize=(9, 6))\n","plt.title(f\"frame {ann_frame_idx}\")\n","plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n","show_points(points, labels, plt.gca())\n","for i, out_obj_id in enumerate(out_obj_ids):\n","    show_points(*prompts[out_obj_id], plt.gca())\n","    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"],"metadata":{"id":"EOQwEqbxkCDc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add the first object\n","ann_frame_idx = 0  # the frame index we interact with\n","ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n","\n","# Let's add a 2nd negative click at (x, y) = (275, 175) to refine the first object\n","# sending all clicks (and their labels) to `add_new_points_or_box`\n","points = np.array([[150, 300], [600, 100]], dtype=np.float32)\n","# for labels, `1` means positive click and `0` means negative click\n","labels = np.array([1, 0], np.int32)\n","prompts[ann_obj_id] = points, labels\n","_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n","    inference_state=inference_state,\n","    frame_idx=ann_frame_idx,\n","    obj_id=ann_obj_id,\n","    points=points,\n","    labels=labels,\n",")\n","\n","# show the results on the current (interacted) frame\n","plt.figure(figsize=(9, 6))\n","plt.title(f\"frame {ann_frame_idx}\")\n","plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n","show_points(points, labels, plt.gca())\n","for i, out_obj_id in enumerate(out_obj_ids):\n","    show_points(*prompts[out_obj_id], plt.gca())\n","    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"],"metadata":{"id":"BL82Ed0zkDW0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ann_frame_idx = 0  # the frame index we interact with\n","ann_obj_id = 3  # give a unique id to each object we interact with (it can be any integers)\n","\n","# Let's now move on to the second object we want to track (giving it object id `3`)\n","# with a positive click at (x, y) = (400, 150)\n","points = np.array([[350, 350]], dtype=np.float32)\n","# for labels, `1` means positive click and `0` means negative click\n","labels = np.array([1], np.int32)\n","prompts[ann_obj_id] = points, labels\n","\n","# `add_new_points_or_box` returns masks for all objects added so far on this interacted frame\n","_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n","    inference_state=inference_state,\n","    frame_idx=ann_frame_idx,\n","    obj_id=ann_obj_id,\n","    points=points,\n","    labels=labels,\n",")\n","\n","# show the results on the current (interacted) frame on all objects\n","plt.figure(figsize=(9, 6))\n","plt.title(f\"frame {ann_frame_idx}\")\n","plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n","show_points(points, labels, plt.gca())\n","for i, out_obj_id in enumerate(out_obj_ids):\n","    show_points(*prompts[out_obj_id], plt.gca())\n","    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"],"metadata":{"id":"KSPGEmLXkFzu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run propagation throughout the video and collect the results in a dict\n","video_segments = {}  # video_segments contains the per-frame segmentation results\n","for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n","    video_segments[out_frame_idx] = {\n","        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n","        for i, out_obj_id in enumerate(out_obj_ids)\n","    }\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from google.colab import files\n","\n","# Create output folder\n","output_dir = \"/content/output_frames\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Parameters\n","vis_frame_stride = 5\n","output_video_path = \"/content/segmentation_output.mp4\"\n","\n","# Save each masked frame\n","plt.close(\"all\")\n","for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n","    fig = plt.figure(figsize=(6, 4))\n","    plt.title(f\"frame {out_frame_idx}\")\n","\n","    img = Image.open(os.path.join(video_dir, frame_names[out_frame_idx]))\n","    plt.imshow(img)\n","\n","    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n","\n","    save_path = os.path.join(output_dir, f\"frame_{out_frame_idx:04d}.png\")\n","    fig.savefig(save_path)\n","    plt.close(fig)\n","\n","# Create video from saved frames\n","image_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.png')])\n","first_frame = cv2.imread(os.path.join(output_dir, image_files[0]))\n","height, width, _ = first_frame.shape\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","video_writer = cv2.VideoWriter(output_video_path, fourcc, 10, (width, height))\n","\n","for image_file in image_files:\n","    frame = cv2.imread(os.path.join(output_dir, image_file))\n","    video_writer.write(frame)\n","video_writer.release()\n","\n","# Download the video\n","files.download(output_video_path)\n","\n","\n","\n","# render the segmentation results every few frames\n","vis_frame_stride = 30\n","plt.close(\"all\")\n","for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n","    plt.figure(figsize=(6, 4))\n","    plt.title(f\"frame {out_frame_idx}\")\n","    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n","    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n","        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"],"metadata":{"id":"u11EQMpvkH5b"},"execution_count":null,"outputs":[]}]}